{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsnCPbdkxYZd"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1 style=\"color: #FF6347;\">Self-Guided Lab: Retrieval-Augmented Generation (RAGs)</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZp4BQAVxYZj"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ3FsdzRveTBrenMxM3VnbDMwaTJxN2NnZm50aGFibXk1NzNnY2Q0MCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LR5ZBwZHv02lmpVoEU/giphy.gif\" alt=\"NLP Gif\" style=\"width: 300px; height: 150px; object-fit: cover; object-position: center;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizk6HCYxYZo"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Data Storage & Retrieval</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5UOI8ZxYZp"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">PyPDFLoader</h2>\n",
        "\n",
        "`PyPDFLoader` is a lightweight Python library designed to streamline the process of loading and parsing PDF documents for text processing tasks. It is particularly useful in Retrieval-Augmented Generation workflows where text extraction from PDFs is required.\n",
        "\n",
        "- **What Does PyPDFLoader Do?**\n",
        "  - Extracts text from PDF files, retaining formatting and layout.\n",
        "  - Simplifies the preprocessing of document-based datasets.\n",
        "  - Supports efficient and scalable loading of large PDF collections.\n",
        "\n",
        "- **Key Features:**\n",
        "  - Compatible with popular NLP libraries and frameworks.\n",
        "  - Handles multi-page PDFs and embedded images (e.g., OCR-compatible setups).\n",
        "  - Provides flexible configurations for structured text extraction.\n",
        "\n",
        "- **Use Cases:**\n",
        "  - Preparing PDF documents for retrieval-based systems in RAGs.\n",
        "  - Automating the text extraction pipeline for document analysis.\n",
        "  - Creating datasets from academic papers, technical manuals, and reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pypdf\n",
            "  Using cached pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Using cached langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
            "  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting langsmith>=0.1.17 (from langchain)\n",
            "  Using cached langsmith-0.4.25-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.43-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
            "  Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain_community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain_community)\n",
            "  Using cached pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Using cached httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain_community) (1.26.4)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl.metadata (43 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.24.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
            "Collecting anyio (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
            "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ------------------------------------- -- 1.8/2.0 MB 12.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 1.8/2.0 MB 12.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.0/2.0 MB 3.6 MB/s  0:00:00\n",
            "Downloading sqlalchemy-2.0.43-cp311-cp311-win_amd64.whl (2.1 MB)\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------  2.1/2.1 MB 10.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.1/2.1 MB 8.5 MB/s  0:00:00\n",
            "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Using cached langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "Downloading aiohttp-3.12.15-cp311-cp311-win_amd64.whl (453 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading multidict-6.6.4-cp311-cp311-win_amd64.whl (46 kB)\n",
            "Using cached pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
            "Using cached pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
            "Downloading greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached langsmith-0.4.25-py3-none-any.whl (379 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading orjson-3.11.3-cp311-cp311-win_amd64.whl (131 kB)\n",
            "Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading zstandard-0.24.0-cp311-cp311-win_amd64.whl (505 kB)\n",
            "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: zstandard, typing-inspection, tenacity, sniffio, python-dotenv, pypdf, pydantic-core, propcache, orjson, mypy-extensions, multidict, marshmallow, jsonpointer, httpx-sse, h11, greenlet, frozenlist, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic, jsonpatch, httpcore, anyio, aiosignal, pydantic-settings, httpx, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "\n",
            "   -- -------------------------------------  2/38 [tenacity]\n",
            "   ---- -----------------------------------  4/38 [python-dotenv]\n",
            "   ----- ----------------------------------  5/38 [pypdf]\n",
            "   ----- ----------------------------------  5/38 [pypdf]\n",
            "   ----- ----------------------------------  5/38 [pypdf]\n",
            "   ----- ----------------------------------  5/38 [pypdf]\n",
            "   ------- --------------------------------  7/38 [propcache]\n",
            "   ---------- ----------------------------- 10/38 [multidict]\n",
            "   ------------ --------------------------- 12/38 [jsonpointer]\n",
            "   -------------- ------------------------- 14/38 [h11]\n",
            "   --------------- ------------------------ 15/38 [greenlet]\n",
            "   --------------- ------------------------ 15/38 [greenlet]\n",
            "   ----------------- ---------------------- 17/38 [attrs]\n",
            "   -------------------- ------------------- 19/38 [aiohappyeyeballs]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ----------------------- ---------------- 22/38 [SQLAlchemy]\n",
            "   ------------------------ --------------- 23/38 [requests-toolbelt]\n",
            "   ------------------------ --------------- 23/38 [requests-toolbelt]\n",
            "   ------------------------- -------------- 24/38 [pydantic]\n",
            "   ------------------------- -------------- 24/38 [pydantic]\n",
            "   ------------------------- -------------- 24/38 [pydantic]\n",
            "   ------------------------- -------------- 24/38 [pydantic]\n",
            "   ------------------------- -------------- 24/38 [pydantic]\n",
            "   ------------------------- -------------- 24/38 [pydantic]\n",
            "   --------------------------- ------------ 26/38 [httpcore]\n",
            "   ---------------------------- ----------- 27/38 [anyio]\n",
            "   ---------------------------- ----------- 27/38 [anyio]\n",
            "   ---------------------------- ----------- 27/38 [anyio]\n",
            "   ------------------------------ --------- 29/38 [pydantic-settings]\n",
            "   ------------------------------- -------- 30/38 [httpx]\n",
            "   ------------------------------- -------- 30/38 [httpx]\n",
            "   --------------------------------- ------ 32/38 [aiohttp]\n",
            "   --------------------------------- ------ 32/38 [aiohttp]\n",
            "   --------------------------------- ------ 32/38 [aiohttp]\n",
            "   --------------------------------- ------ 32/38 [aiohttp]\n",
            "   ---------------------------------- ----- 33/38 [langsmith]\n",
            "   ---------------------------------- ----- 33/38 [langsmith]\n",
            "   ---------------------------------- ----- 33/38 [langsmith]\n",
            "   ---------------------------------- ----- 33/38 [langsmith]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ----------------------------------- ---- 34/38 [langchain-core]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   ------------------------------------- -- 36/38 [langchain]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   -------------------------------------- - 37/38 [langchain_community]\n",
            "   ---------------------------------------- 38/38 [langchain_community]\n",
            "\n",
            "Successfully installed SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.10.0 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.7.0 greenlet-3.2.4 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-core-0.3.75 langchain-text-splitters-0.3.11 langchain_community-0.3.29 langsmith-0.4.25 marshmallow-3.26.1 multidict-6.6.4 mypy-extensions-1.1.0 orjson-3.11.3 propcache-0.3.2 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 pypdf-6.0.0 python-dotenv-1.1.1 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 typing-inspect-0.9.0 typing-inspection-0.4.1 yarl-1.20.1 zstandard-0.24.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting termcolor\n",
            "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting langchain_openai\n",
            "  Using cached langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Using cached langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (5.1.0)\n",
            "Collecting chromadb\n",
            "  Using cached chromadb-1.0.20-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
            "Collecting langchain_chroma\n",
            "  Using cached langchain_chroma-0.2.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting openai\n",
            "  Using cached openai-1.106.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain_openai) (0.3.75)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from tiktoken) (2025.9.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from openai) (4.10.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from openai) (0.28.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.10.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.25)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-huggingface) (0.22.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langchain-huggingface) (0.34.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from sentence-transformers) (4.56.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.7.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Using cached build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting importlib-resources (from chromadb)\n",
            "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting grpcio>=1.58.0 (from chromadb)\n",
            "  Downloading grpcio-1.74.0-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
            "Collecting typer>=0.9.0 (from chromadb)\n",
            "  Downloading typer-0.17.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from chromadb) (3.11.3)\n",
            "Collecting rich>=10.11.0 (from chromadb)\n",
            "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting jsonschema>=4.19.0 (from chromadb)\n",
            "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
            "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
            "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb)\n",
            "  Downloading rpds_py-0.27.1-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
            "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.24.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached protobuf-6.32.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: sympy in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Using cached opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
            "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: networkx in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vinyj\\anaconda3\\envs\\nlp311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
            "Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl (884 kB)\n",
            "   ---------------------------------------- 0.0/884.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 884.4/884.4 kB 8.0 MB/s  0:00:00\n",
            "Using cached openai-1.106.1-py3-none-any.whl (930 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.10.0-cp311-cp311-win_amd64.whl (209 kB)\n",
            "Using cached langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Using cached chromadb-1.0.20-cp39-abi3-win_amd64.whl (19.8 MB)\n",
            "Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "Using cached langchain_chroma-0.2.5-py3-none-any.whl (12 kB)\n",
            "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
            "Using cached build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading grpcio-1.74.0-cp311-cp311-win_amd64.whl (4.5 MB)\n",
            "   ---------------------------------------- 0.0/4.5 MB ? eta -:--:--\n",
            "   --------------------------- ------------ 3.1/4.5 MB 16.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.5/4.5 MB 12.9 MB/s  0:00:00\n",
            "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
            "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "Using cached durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading mmh3-5.2.0-cp311-cp311-win_amd64.whl (41 kB)\n",
            "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl (12.7 MB)\n",
            "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 3.1/12.7 MB 15.4 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 6.6/12.7 MB 15.5 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 10.2/12.7 MB 15.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.6/12.7 MB 14.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.7/12.7 MB 14.0 MB/s  0:00:00\n",
            "Using cached opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "Using cached opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Using cached opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Using cached opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Using cached opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "Using cached opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "Using cached protobuf-6.32.0-cp310-abi3-win_amd64.whl (435 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading pybase64-1.4.2-cp311-cp311-win_amd64.whl (35 kB)\n",
            "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
            "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
            "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading rpds_py-0.27.1-cp311-cp311-win_amd64.whl (228 kB)\n",
            "Downloading typer-0.17.3-py3-none-any.whl (46 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
            "Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl (292 kB)\n",
            "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
            "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
            "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml): started\n",
            "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=b62cc4469dd4b77467a7ccc0233eaf4dc9bdeee16a2662a4aeb194b421359666\n",
            "  Stored in directory: c:\\users\\vinyj\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, flatbuffers, durationpy, websockets, websocket-client, termcolor, shellingham, rpds-py, pyreadline3, pyproject_hooks, pybase64, pyasn1, protobuf, overrides, oauthlib, mmh3, mdurl, jiter, importlib-resources, httptools, grpcio, distro, cachetools, bcrypt, backoff, watchfiles, uvicorn, tiktoken, rsa, requests-oauthlib, referencing, pyasn1-modules, posthog, opentelemetry-proto, opentelemetry-api, markdown-it-py, humanfriendly, googleapis-common-protos, build, rich, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, openai, jsonschema-specifications, google-auth, coloredlogs, typer, opentelemetry-sdk, onnxruntime, kubernetes, jsonschema, opentelemetry-exporter-otlp-proto-grpc, langchain_openai, langchain-huggingface, chromadb, langchain_chroma\n",
            "\n",
            "   ----------------------------------------  0/56 [pypika]\n",
            "   - --------------------------------------  2/56 [durationpy]\n",
            "   -- -------------------------------------  3/56 [websockets]\n",
            "   -- -------------------------------------  3/56 [websockets]\n",
            "   -- -------------------------------------  4/56 [websocket-client]\n",
            "   ---- -----------------------------------  6/56 [shellingham]\n",
            "   ----- ----------------------------------  8/56 [pyreadline3]\n",
            "   ----- ----------------------------------  8/56 [pyreadline3]\n",
            "   ------- -------------------------------- 10/56 [pybase64]\n",
            "   ------- -------------------------------- 11/56 [pyasn1]\n",
            "   -------- ------------------------------- 12/56 [protobuf]\n",
            "   -------- ------------------------------- 12/56 [protobuf]\n",
            "   -------- ------------------------------- 12/56 [protobuf]\n",
            "   --------- ------------------------------ 13/56 [overrides]\n",
            "   ---------- ----------------------------- 14/56 [oauthlib]\n",
            "   ---------- ----------------------------- 14/56 [oauthlib]\n",
            "   ---------- ----------------------------- 15/56 [mmh3]\n",
            "   ------------ --------------------------- 18/56 [importlib-resources]\n",
            "   ------------- -------------------------- 19/56 [httptools]\n",
            "   -------------- ------------------------- 20/56 [grpcio]\n",
            "   -------------- ------------------------- 20/56 [grpcio]\n",
            "   --------------- ------------------------ 21/56 [distro]\n",
            "   ----------------- ---------------------- 24/56 [backoff]\n",
            "   ------------------ --------------------- 26/56 [uvicorn]\n",
            "   ------------------ --------------------- 26/56 [uvicorn]\n",
            "   -------------------- ------------------- 28/56 [rsa]\n",
            "   -------------------- ------------------- 29/56 [requests-oauthlib]\n",
            "   ---------------------- ----------------- 31/56 [pyasn1-modules]\n",
            "   ---------------------- ----------------- 31/56 [pyasn1-modules]\n",
            "   ---------------------- ----------------- 31/56 [pyasn1-modules]\n",
            "   ---------------------- ----------------- 31/56 [pyasn1-modules]\n",
            "   ---------------------- ----------------- 31/56 [pyasn1-modules]\n",
            "   ---------------------- ----------------- 32/56 [posthog]\n",
            "   ---------------------- ----------------- 32/56 [posthog]\n",
            "   ----------------------- ---------------- 33/56 [opentelemetry-proto]\n",
            "   ------------------------ --------------- 34/56 [opentelemetry-api]\n",
            "   ------------------------- -------------- 35/56 [markdown-it-py]\n",
            "   ------------------------- -------------- 35/56 [markdown-it-py]\n",
            "   ------------------------- -------------- 35/56 [markdown-it-py]\n",
            "   -------------------------- ------------- 37/56 [googleapis-common-protos]\n",
            "   -------------------------- ------------- 37/56 [googleapis-common-protos]\n",
            "   -------------------------- ------------- 37/56 [googleapis-common-protos]\n",
            "   -------------------------- ------------- 37/56 [googleapis-common-protos]\n",
            "   --------------------------- ------------ 39/56 [rich]\n",
            "   --------------------------- ------------ 39/56 [rich]\n",
            "   --------------------------- ------------ 39/56 [rich]\n",
            "   --------------------------- ------------ 39/56 [rich]\n",
            "   ----------------------- --------- 40/56 [opentelemetry-semantic-conventions]\n",
            "   ----------------------- --------- 40/56 [opentelemetry-semantic-conventions]\n",
            "   ----------------------- --------- 40/56 [opentelemetry-semantic-conventions]\n",
            "   ----------------------- --------- 40/56 [opentelemetry-semantic-conventions]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 42/56 [openai]\n",
            "   ------------------------------ --------- 43/56 [jsonschema-specifications]\n",
            "   ------------------------------- -------- 44/56 [google-auth]\n",
            "   ------------------------------- -------- 44/56 [google-auth]\n",
            "   ------------------------------- -------- 44/56 [google-auth]\n",
            "   -------------------------------- ------- 46/56 [typer]\n",
            "   --------------------------------- ------ 47/56 [opentelemetry-sdk]\n",
            "   --------------------------------- ------ 47/56 [opentelemetry-sdk]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ---------------------------------- ----- 48/56 [onnxruntime]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 49/56 [kubernetes]\n",
            "   ----------------------------------- ---- 50/56 [jsonschema]\n",
            "   ----------------------------------- ---- 50/56 [jsonschema]\n",
            "   ------------------------------------- -- 52/56 [langchain_openai]\n",
            "   ------------------------------------- -- 53/56 [langchain-huggingface]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   -------------------------------------- - 54/56 [chromadb]\n",
            "   ---------------------------------------  55/56 [langchain_chroma]\n",
            "   ---------------------------------------- 56/56 [langchain_chroma]\n",
            "\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 cachetools-5.5.2 chromadb-1.0.20 coloredlogs-15.0.1 distro-1.9.0 durationpy-0.10 flatbuffers-25.2.10 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.74.0 httptools-0.6.4 humanfriendly-10.0 importlib-resources-6.5.2 jiter-0.10.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 kubernetes-33.1.0 langchain-huggingface-0.3.1 langchain_chroma-0.2.5 langchain_openai-0.3.32 markdown-it-py-4.0.0 mdurl-0.1.2 mmh3-5.2.0 oauthlib-3.3.1 onnxruntime-1.22.1 openai-1.106.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 overrides-7.7.0 posthog-5.4.0 protobuf-6.32.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 referencing-0.36.2 requests-oauthlib-2.0.0 rich-14.1.0 rpds-py-0.27.1 rsa-4.9.1 shellingham-1.5.4 termcolor-3.1.0 tiktoken-0.11.0 typer-0.17.3 uvicorn-0.35.0 watchfiles-1.1.0 websocket-client-1.8.0 websockets-15.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain langchain_community pypdf\n",
        "%pip install termcolor langchain_openai langchain-huggingface sentence-transformers chromadb langchain_chroma tiktoken openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6heKZkQUxYZr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRS44B2XxYZs",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Loading the Documents</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cuREtJRixYZt"
      },
      "outputs": [],
      "source": [
        "# File path for the document\n",
        "\n",
        "file_path = \"ai-for-everyone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz_8SOLxxYZt"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Documents into pages</h3>\n",
        "\n",
        "The `PyPDFLoader` library allows efficient loading and splitting of PDF documents into smaller, manageable parts for NLP tasks.\n",
        "\n",
        "This functionality is particularly useful in workflows requiring granular text processing, such as Retrieval-Augmented Generation (RAG).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_b5Z_45UxYZu",
        "outputId": "a600d69f-14fe-4492-f236-97261d6ff36c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "297"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and split the document\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = loader.load_and_split()\n",
        "len(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt50NRQaxYZv"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Pages into Chunks</h3>\n",
        "\n",
        "\n",
        "####  RecursiveCharacterTextSplitter in LangChain\n",
        "\n",
        "The `RecursiveCharacterTextSplitter` is the **recommended splitter** in LangChain when you want to break down long documents into smaller, semantically meaningful chunks — especially useful in **RAG pipelines**, where clean context chunks lead to better LLM responses.\n",
        "\n",
        "####  Parameters\n",
        "\n",
        "| Parameter       | Description                                                                 |\n",
        "|-----------------|-----------------------------------------------------------------------------|\n",
        "| `chunk_size`    | The **maximum number of characters** allowed in a chunk (e.g., `1000`).     |\n",
        "| `chunk_overlap` | The number of **overlapping characters** between consecutive chunks (e.g., `200`). This helps preserve context continuity. |\n",
        "\n",
        "####  How it works\n",
        "`RecursiveCharacterTextSplitter` attempts to split the text **intelligently**, trying the following separators in order:\n",
        "1. Paragraphs (`\"\\n\\n\"`)\n",
        "2. Lines (`\"\\n\"`)\n",
        "3. Sentences or words (`\" \"`)\n",
        "4. Individual characters (as a last resort)\n",
        "\n",
        "This makes it ideal for handling **natural language documents**, such as PDFs, articles, or long reports, without breaking sentences or paragraphs in awkward ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1096"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Alternative: CharacterTextSplitter\n",
        "\n",
        "`CharacterTextSplitter` is a simpler splitter that breaks text into chunks based **purely on character count**, without trying to preserve any natural language structure.\n",
        "\n",
        "##### Example:\n",
        "```python\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "````\n",
        "\n",
        "This method is faster and more predictable but may split text in the middle of a sentence or paragraph, which can hurt performance in downstream tasks like retrieval or QA.\n",
        "\n",
        "---\n",
        "\n",
        "#### Comparison Table\n",
        "\n",
        "| Feature                        | RecursiveCharacterTextSplitter | CharacterTextSplitter     |\n",
        "| ------------------------------ | ------------------------------ | ------------------------- |\n",
        "| Structure-aware splitting      |  Yes                          |  No                      |\n",
        "| Preserves sentence/paragraphs  |  Yes                          |  No                      |\n",
        "| Risk of splitting mid-sentence |  Minimal                     |  High                   |\n",
        "| Ideal for RAG/document QA      |  Highly recommended           |  Only if structured text |\n",
        "| Performance speed              |  Slightly slower             |  Faster                  |\n",
        "\n",
        "---\n",
        "\n",
        "#### Recommendation\n",
        "\n",
        "Use `RecursiveCharacterTextSplitter` for most real-world document processing tasks, especially when building RAG pipelines or working with structured natural language content like PDFs or articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Choosing Chunk Size in RAG\n",
        "\n",
        "### Best Practices for Chunk Size in RAG\n",
        "\n",
        "| Factor                      | Recommendation                                                                                                                                                                                          |\n",
        "| ---------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **LLM context limit**       | Choose a chunk size that lets you retrieve multiple chunks **without exceeding the model’s token limit**. For example, GPT-4o supports 128k tokens, but with GPT-3.5 (16k) or GPT-4 (32k), keep it modest. |\n",
        "| **Chunk size (in characters)** | Typically: **500–1,000 characters** per chunk → ~75–200 tokens. This fits well for retrieval + prompt without context overflow.                                                                           |\n",
        "| **Chunk size (in tokens)**  | If using token-based splitter (e.g. `TokenTextSplitter`): aim for **100–300 tokens** per chunk.                                                                                                            |\n",
        "| **Chunk overlap**           | Use **overlap of 10–30%** (e.g., 100–300 characters or ~50 tokens) to preserve context across chunk boundaries and avoid cutting off important ideas mid-sentence.                                        |\n",
        "| **Document structure**      | Use **`RecursiveCharacterTextSplitter`** to preserve semantic boundaries (paragraphs, sentences) instead of arbitrary cuts.                                                                                |\n",
        "| **Task type**               | For **question answering**, smaller chunks (~500–800 chars) reduce noise.<br>For **summarization**, slightly larger chunks (~1000–1500) are OK.                                                          |\n",
        "| **Embedding model**         | Some models (e.g., `text-embedding-3-large`) can handle long input. But still, smaller chunks give **finer-grained retrieval**, which improves relevance.                                                  |\n",
        "| **Query type**              | If users ask **very specific questions**, small focused chunks are better. For broader queries, bigger chunks might help.                                                                                  |\n",
        "\n",
        "\n",
        "### Rule of Thumb\n",
        "\n",
        "| Use Case                 | Chunk Size      | Overlap |\n",
        "| ------------------------| --------------- | ------- |\n",
        "| Factual Q&A              | 500–800 chars   | 100–200 |\n",
        "| Summarization            | 1000–1500 chars | 200–300 |\n",
        "| Technical documents      | 400–700 chars   | 100–200 |\n",
        "| Long reports/books       | 800–1200 chars  | 200–300 |\n",
        "| Small LLMs (≤16k tokens) | ≤800 chars      | 100–200 |\n",
        "\n",
        "\n",
        "### Avoid\n",
        "\n",
        "- Chunks >2000 characters: risks context overflow.\n",
        "- No overlap: may lose key information between chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg15RjVPxYZw"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Embeddings</h2>\n",
        "\n",
        "Embeddings transform text into dense vector representations, capturing semantic meaning and contextual relationships. They are essential for efficient document retrieval and similarity analysis.\n",
        "\n",
        "- **What are OpenAI Embeddings?**\n",
        "  - Pre-trained embeddings like `text-embedding-3-large` generate high-quality vector representations for text.\n",
        "  - Encapsulate semantic relationships in the text, enabling robust NLP applications.\n",
        "\n",
        "- **Key Features of `text-embedding-3-large`:**\n",
        "  - Large-scale embedding model optimized for accuracy and versatility.\n",
        "  - Handles diverse NLP tasks, including retrieval, classification, and clustering.\n",
        "  - Ideal for applications with high-performance requirements.\n",
        "\n",
        "- **Benefits:**\n",
        "  - Reduces the need for extensive custom training.\n",
        "  - Provides state-of-the-art performance in retrieval-augmented systems.\n",
        "  - Compatible with RAGs to create powerful context-aware models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "L0xDxElwxYZw"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_WRIo3_0xYZx",
        "outputId": "78bfbbf3-9d25-4e31-bdbc-3e932e6bbfec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MNZfTng5xYZz",
        "outputId": "db1a7c85-ef9f-447e-92cd-9d097e959847"
      },
      "outputs": [],
      "source": [
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsSA7RKvxYZz"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChromaDB</h2>\n",
        "\n",
        "ChromaDB is a versatile vector database designed for efficiently storing and retrieving embeddings. It integrates seamlessly with embedding models to enable high-performance similarity search and context-based retrieval.\n",
        "\n",
        "### Workflow Overview:\n",
        "- **Step 1:** Generate embeddings using a pre-trained model (e.g., OpenAI's `text-embedding-3-large`).\n",
        "- **Step 2:** Store the embeddings in ChromaDB for efficient retrieval and similarity calculations.\n",
        "- **Step 3:** Use the stored embeddings to perform searches, matching, or context-based retrieval.\n",
        "\n",
        "### Key Features of ChromaDB:\n",
        "- **Scalability:** Handles large-scale datasets with optimized indexing and search capabilities.\n",
        "- **Speed:** Provides fast and accurate retrieval of embeddings for real-time applications.\n",
        "- **Integration:** Supports integration with popular frameworks and libraries for embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "brKe6wUgxYZ0"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VkjHR-RkxYZ0",
        "outputId": "bc11bda9-f283-457a-f584-5a06b95c4dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB created with document embeddings.\n"
          ]
        }
      ],
      "source": [
        "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db_LAB\")\n",
        "print(\"ChromaDB created with document embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27OdN1IVxYZ1"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Retrieving Documents</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice1: Write a user question that someone might ask about your book’s topic or content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XiLv-TfrxYZ1"
      },
      "outputs": [],
      "source": [
        "user_question = \"Could a humanbe smarter than AI?\" # User question\n",
        "retrieved_docs = db.similarity_search(user_question, k=10) # k is the number of documents to retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "qgWsh50JxYZ1",
        "outputId": "c8640c5d-5955-471f-fdd2-37096f5f68c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            "uman species (The Economist 2020b; Bostrom 2014). Stephen \n",
            "Hawking has pronounced to the BBC that ‘the development of full artificial \n",
            "intelligence could spell the end of the human race’ (Cellan-Jones 2014, n.p.).  \n",
            "AI represents profound fears – culminating in our extinction – but also  \n",
            "profound hopes in bettering life for humanity and life on Earth. \n",
            "For these reasons – to ward off our fears and harness AI for betterment – \n",
            "the need to continually push for deeper understanding of AI is also corre -\n",
            "spondingly clear. The current rationalisation of AI persists in human terms, \n",
            "as is evident from even the most recent musings on the limitations of AI (The \n",
            "Economist 2020a): comparisons are consistently made with human learning \n",
            "and cognition, such as ‘embodied cognition’ , or references to the ‘irredeemably \n",
            "complex’ nature of human minds (n.p.). In filling the gap of understanding why\n",
            "Document 2:\n",
            "ich is quite a tough question to answer. \n",
            "Most likely, humans will always have exclusivity when it comes to artistic crea-\n",
            "tivity, Albert Einstein having pointed out that ‘creativity is intelligence having \n",
            "fun’ . Currently, it seems very improbable that AI systems will be able to be truly \n",
            "creative. But then again, the question is what exactly true creativity is, and who \n",
            "will be the judge of it?\n",
            "Artificial Intelligence: History and Evolution\n",
            "To structure AI’s history, we’ll use an analogy of the four seasons: spring, sum-\n",
            "mer, autumn and winter (Haenlein and Kaplan 2019). AI’s birth period, i.e., \n",
            "spring, took place both in fiction as well as non-fiction. Regarding the former, \n",
            "Isaac Asimov, an American writer and professor of biochemistry at Boston Uni-\n",
            "versity, published ‘Runaround’ , a story revolving around an AI-driven robot, in \n",
            "1942. In this story, Asimov’s (1950, 40) three laws of robotics explicitly appear \n",
            "for the first time:\n",
            "Document 3:\n",
            "l increase in computing required for the \n",
            "latest deep learning models and the consequent carbon emissions (Strubell, \n",
            "Ganesh and McCallum 2019). But humanism itself, as the vision of the human \n",
            "as separate and subject to special rules, is the precursor of worldviews that have \n",
            "created the possibility of the Anthropocene.\n",
            "Perhaps the most immediately dangerous aspect of human exceptionalism \n",
            "is the one linked directly to the definition of AI; the question of consciousness \n",
            "and superior intelligence. Humanism sees the spark of rational intelligence as a \n",
            "marker of uniqueness. The field of AI meanwhile, while its current best practice \n",
            "is the steamhammer of statistical prediction, still holds on to the idea that this \n",
            "narrow form of computational ‘intelligence’ is a foothill on the way to artificial \n",
            "general intelligence; that is, machines that can think like us (Hodson 2016).\n"
          ]
        }
      ],
      "source": [
        "# Display top results\n",
        "for i, doc in enumerate(retrieved_docs[:3]): # Display top 3 results\n",
        "    print(f\"Document {i+1}:\\n{doc.page_content[36:1000]}\") # Display content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuGK8gL6xYZ1"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Preparing Content for GenAI</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "2iB3lZqHxYZ2"
      },
      "outputs": [],
      "source": [
        "def _get_document_prompt(docs):\n",
        "    prompt = \"\\n\"\n",
        "    for doc in docs:\n",
        "        prompt += \"\\nContent:\\n\"\n",
        "        prompt += doc.page_content + \"\\n\\n\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "2okzmuADxYZ2",
        "outputId": "0aa6cdca-188d-40e0-f5b4-8888d3549ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context formatted for GPT model.\n"
          ]
        }
      ],
      "source": [
        "# Generate a formatted context from the retrieved documents\n",
        "formatted_context = _get_document_prompt(retrieved_docs)\n",
        "print(\"Context formatted for GPT model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzIczQNTxYZ2"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChatBot Architecture</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice2: Write a prompt that is relevant and tailored to the content and style of your book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "tqxVh9s3xYZ3",
        "outputId": "97cca95d-4ab3-44d8-a76c-5713aad387d8"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "\n",
        "# Task\n",
        "Answer the user's question using only the CONTEXT below.\n",
        "\n",
        "# Question\n",
        "{user_question}\n",
        "\n",
        "# Context (retrieved passages)\n",
        "{formatted_context}\n",
        "\n",
        "# Output rules\n",
        "- Use only facts present in the Context; do not invent details.\n",
        "- If multiple passages disagree, note the disagreement briefly.\n",
        "- Quote short phrases if key, then explain in your own words.\n",
        "- If the context is insufficient, reply: \"I don't know based on the provided context.\"\n",
        "- Tone: concise, professional. Limit to {1500} words.\n",
        "- If a list is natural, use bullet points (max 7 items).\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "0mjkQJ_ZxYZ3"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice3: Tune parameters like temperature, and penalties to control how creative, focused, or varied the model's responses are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ylypRWRlxYZ4"
      },
      "outputs": [],
      "source": [
        "# Set up GPT client and parameters\n",
        "client = openai.OpenAI()\n",
        "model_params = {\n",
        "    'model': 'gpt-4o',\n",
        "    'temperature':0.8 ,  # Increase creativity\n",
        "    'max_tokens':1500,  # Allow for longer responses\n",
        "    'top_p': 0.9,        # Use nucleus sampling\n",
        "    'frequency_penalty': 0.5,  # Reduce repetition\n",
        "    'presence_penalty': 0.7   # Encourage new topics\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8e942xDxYZ4"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Response</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "4eXZO4pIxYZ4"
      },
      "outputs": [],
      "source": [
        "messages = [{'role': 'user', 'content': prompt}]\n",
        "completion = client.chat.completions.create(messages=messages, **model_params, timeout=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "wLPAcchBxYZ5",
        "outputId": "976c7800-16ed-41fe-c4cf-58f60d3230d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, it is suggested that while AI has capabilities that can surpass human abilities in certain tasks, such as pattern recognition and data processing with high accuracy, there are areas where humans may retain superiority:\n",
            "\n",
            "- **Artistic Creativity**: The context suggests that humans might always have an exclusivity in artistic creativity. AI currently struggles with what could be considered \"true creativity,\" a concept that remains challenging to define and judge.\n",
            "\n",
            "- **Complex Human Cognition**: Human cognition involves complexities, including emotional understanding and creative thought processes, which are noted as potentially irredeemable by AI systems. The nature of human minds is described as \"irredeemably complex.\"\n",
            "\n",
            "- **Human Emotions and Empathy**: Although AI can recognize emotions via facial recognition or tone of voice, truly understanding these emotions and responding empathetically remains a uniquely human trait.\n",
            "\n",
            "In conclusion, while AI can outperform humans in specific computational tasks and logical operations, aspects like creativity, complex cognition, and emotional intelligence may remain distinctively human qualities.\n"
          ]
        }
      ],
      "source": [
        "answer = completion.choices[0].message.content\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXVNXPwLxYaT"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png\" alt=\"NLP Gif\" style=\"width: 500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldybhlqKxYaT"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Cosine Similarity</h2>\n",
        "\n",
        "**Cosine similarity** is a metric used to measure the alignment or similarity between two vectors, calculated as the cosine of the angle between them. It is the **most common metric used in RAG pipelines** for vector retrieval.. It provides a scale from -1 to 1:\n",
        "\n",
        "- **-1**: Vectors are completely opposite.\n",
        "- **0**: Vectors are orthogonal (uncorrelated or unrelated).\n",
        "- **1**: Vectors are identical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1I1TNhxYaT"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\" alt=\"NLP Gif\" style=\"width: 700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEMdNgQxYaU"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Keyword Highlighting</h2>\n",
        "\n",
        "Highlighting important keywords helps users quickly understand the relevance of the retrieved text to their query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "nCXL9Cz1xYaV"
      },
      "outputs": [],
      "source": [
        "from termcolor import colored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwDyofY0xYaV"
      },
      "source": [
        "The `highlight_keywords` function is designed to highlight specific keywords within a given text. It replaces each keyword in the text with a highlighted version using the `colored` function from the `termcolor` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "9y3E0YWExYaV"
      },
      "outputs": [],
      "source": [
        "def highlight_keywords(text, keywords):\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, colored(keyword, 'green', attrs=['bold']))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice4: add your keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "i7SkWPpnxYaW",
        "outputId": "28e82563-edba-4b41-acad-ec27e5ba134f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snippet 1:\n",
            "62 AI for Everyone?\n",
            "wiping out the \u001b[1m\u001b[32mhuman\u001b[0m species (The Economist 2020b; Bostrom 2014). Stephen \n",
            "Hawking has pronounced to the BBC that ‘the development of full artificial \n",
            "intelligence could spell the \n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "query_keywords = [\"human\", \"robot\"] # add your keywords\n",
        "for i, doc in enumerate(retrieved_docs[:1]):\n",
        "    snippet = doc.page_content[:200]\n",
        "    highlighted = highlight_keywords(snippet, query_keywords)\n",
        "    print(f\"Snippet {i+1}:\\n{highlighted}\\n{'-'*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhV_Jf_LxYaX"
      },
      "source": [
        "1. `query_keywords` is a list of keywords to be highlighted.\n",
        "2. The loop iterates over the first document in retrieved_docs.\n",
        "3. For each document, a snippet of the first 200 characters is extracted.\n",
        "4. The highlight_keywords function is called to highlight the keywords in the snippet.\n",
        "5. The highlighted snippet is printed along with a separator line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBRKysAvxYaX"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Bonus</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj25lCybxYaX"
      },
      "source": [
        "**Try loading one of your own PDF books and go through the steps again to explore how the pipeline works with your content**:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
